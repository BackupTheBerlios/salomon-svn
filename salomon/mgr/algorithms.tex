\section{Drzewa decyzyjne}
% http://www.dbmsmag.com/9807m05.html
% When a businessperson needs to make a decision based on several factors, a
% decision tree can help identify which factors to consider and how each factor
% has historically been associated with different outcomes of the decision. For
% example, in our credit risk case study (See the sidebar Predicting Credit Risk),
% we have data for each applicantýs debt, income, and marital status. A decision
% tree creates a model as either a graphical tree or a set of text rules that can
% predict (classify) each applicant as a good or bad credit risk.

Kiedy biznesmen potrzebuje podj¹æ decyzje bazuj¹c na kilku wspó³czynnikach,
drzewa decyzyjne mog¹ pomoc w wybraniu odpowiednich wspó³czynników oraz 
opowiedzieæ jak dany wspó³czynnik w przesz³oœci wp³ywa³ na decyzje.
Przyk³adowo, podczas obliczania ryzyka kredytowego, posiadamy dla ka¿dego
potencjalnego kredytobiorcy dane o jego kredytach, przychodach oraz jego
statusie materialnym. Drzewa decyzyjne tworz¹ zarówno drzewo w postaci
graficznej, jak zbiór regu³ tekstowych, dziêki którym mo¿emy okreœliæ,
czy dana aplikacja jest obarczona du¿ym lub ma³ym ryzykiem kredytowym.

% A decision tree is a model that is both predictive and descriptive. It is called
% a decision tree because the resulting model is presented in the form of a tree
% structure. (See Figure 1.) The visual presentation makes the decision tree model
% very easy to understand and assimilate. As a result, the decision tree has
% become a very popular data mining technique. Decision trees are most commonly
% used for classification (predicting what group a case belongs to), but can also
% be used for regression (predicting a specific value).

Drzewo decyzyjne jest modelem, który jest zarówno predyktywnym jak i opisowym.
%!!!predictive and descriptive!!!.
Nazywany jest drzewem decyzyjnym, poniewa¿ prezentowany jest w formie
drzewiastej struktury~\ref{fig:sample_tree}. Graficzna reprezentacja powoduje,
¿e zrozumienie go czy porównanie go nie jest trudnym zadaniem. Drzewa decyzyjne
sta³y siê bardzo popularne w technikach ,,data maning''. Drzewa decyzyjne s¹
najpowszechniej u¿ywane do klasyfikacji (predykuj¹ do której grupy nale¿y dany
przypadek), lecz mog¹ byæ równie¿ u¿ywane przy regresji (predykuj¹ wartoœæ).

% The decision tree method encompasses a number of specific algorithms, including
% Classification and Regression Trees (CART), Chi-squared Automatic Interaction
% Detection (CHAID), C4.5 and C5.0 (from work by J. Ross Quinlan of Rulequest
% Research Pty Ltd, in St. Ives, Australia, www.rulequest.com).

Metody drzew decyzyjnych obejmuj¹ wiele algorytmów. Miêdzy innymi drzewa
klasyfikuj¹ce i regresyjne (ang. \emph{Classification and Regression Trees
(CART)}), \emph{Chi-squared Automatic
Interaction Detection (CHAID)}, \emph{C4.5} and \emph{C5.0}.

% Decision trees graphically display the relationships found in data. Most
% products also translate the tree-to-text rules such as If Income = High and
% Years on job > 5 Then Credit risk = Good. In fact, decision tree algorithms are
% very similar to rule induction algorithms which produce rule sets without a
% decision tree.

Drzewa decyzyjne w graficzny sposób obrazuj¹ zale¿noœci wystêpuj¹ce w danych.
Mo¿na je równie¿ przedstawiaæ w postaci regu³ tekstowych np.
\\
\\
\emph{Jeœli Dochód = Wysoki i Lata Pracy $>$ 5 Wtedy Ryzyko kredytowe = Ma³e}.
\\
\\
Algorytmy drzew decyzyjnych s¹ bardzo podobne do algorytmów indukcji regu³, które
produkuj¹ zbiory regu³ bez drzew decyzyjnych.

% The primary output of a decision tree algorithm is the tree itself. The 
% training process that creates the decision tree is usually called induction. 
% Induction requires a small number of passes (generally far fewer than 100) 
% through the training dataset. This makes the algorithm somewhat less efficient 
% than Naýve-Bayes algorithms (See Naýve-Bayes and Nearest Neighbor.), which 
% require only one pass, but significantly more efficient than neural nets, which 
% typically require a large number of passes, sometimes numbering in the 
% thousands. To be more precise, the number of passes required to build a 
% decision tree is no more than the number of levels in the tree. There is no 
% predetermined limit to the number of levels, although the complexity of the 
% tree as measured by the depth and breadth of the tree generally increases as 
% the number of independent variables increases.

G³ówny rezultatem algorytmu drzew decyzyjnych jest w³aœnie drzewo. Proces
treningu tworzy drzewo, jest zwyczajowy nazywany indukcj¹. Indukcja wymaga kilku
przejœæ (generalnie znacznie mniej ni¿ 100) przez zbioru trenuj¹cy (eg.
training dataset). Powoduje to, ¿e algorytmy te s¹ mniej wydajne ni¿ algorytmy 
\emph{Naýve-Bayes}, które wymagaj¹ tylko jednego przejœcia, ale znacznie wydajne
ni¿ sieci neuronowe, które zazwyczaj wymaj¹ wielkiej iloœci przejœæ, czasami
liczonych w tysi¹cach. Dok³adniej iloœæ iloœæ potrzebnych przejœæ wymaganych do
zbudowania drzewa decyzyjnego jest nie wiêksza ni¿ wysokoœæ drzewa (iloœæ
warstw). Nie istnieje okreœlona z góry maksymalna wysokoœæ drzewa, jednak¿e
z³o¿onoœæ drzewa mierzona jako jego wysokoœæ i szerokoœæ generalnie roœnie jeœli
roœnie iloœæ niezale¿nych zmiennych.


%\usepackage{graphics} is needed for \includegraphics
\begin{figure}[htp]
\begin{center}
  \includegraphics[width=0.3\textwidth]{img/sample_tree.jpg}
  \caption[labelInTOC]{Drzewo decyzyjne}
  \label{fig:sample_tree}
\end{center}
\end{figure}

\subsection{Omówienie algorytmów}

\subsubsection{ID3}

\subsubsection{C4.5}
