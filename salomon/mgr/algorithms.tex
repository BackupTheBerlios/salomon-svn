
\emph{ID3} i \emph{C4.5} to algorytmy wprowadzone przez \emph{J. Ross Quinlan'a}
do tworzenia na podstawie danych indukcyjnych modeli klasyfikuj¹cych, zwanych tak¿e
drzewami decyzyjnymi. 

Jako wejœcie otrzymuj¹ one zbiór rekordów o jednakowej strukturze, sk³adaj¹cy siê
z pewnej liczby par atrybut-wartoœæ. Jeden z tych atrybutów reprezentuje
tzw. kategoriê rekordu, jest te¿ okreœlany mianem atrybutu decyzyjnego.

Celem algorytmów jest stworzenie drzewa, które na podstawie wartoœci
pozosta³ych atrybutów okreœli wartoœæ atrybutu decyzyjnego.
Zwykle atrybut decyzyjny przyjmuje wartoœci logiczne \emph{(prawda, fa³sz)}, 
\emph{(sukces/pora¿ka)} lub podobne. W ka¿dym przypadku jedna z wartoœci
atrybutu oznacza pora¿kê.


\paragraph{Algorytm ID3}

%http://www.cise.ufl.edu/~ddd/cap6635/Fall-97/Short-papers/2.htm
Algorytm \emph{ID3} tworzy drzewo decyzyjne na podstawie zbioru danych przyk³adowych.
Utworzone w ten sposób drzewo u¿ywne jest do póŸniejszego klasyfikowania
zbiorów zawieraj¹cych podobne dane.

Ka¿dy liœæ drzewa odpowiada podjêtej decyzji (czyli wartoœci atrybutu
decyzyjnego), natomiast wêz³y reprezentuj¹ testy wartoœci pozosta³ych
atrybutów, wp³ywaj¹cych na decyzjê.

Algorytm \emph{ID3} przedstawiony zosta³ przez \emph{J. Ross Quinlan'a} w roku
1975. Bazuje on na algorytmie \emph{CLS (ang. Concept Learning System)}, który w
uproszczeniu dzia³a nastêpuj¹co:

\begin{enumerate}
  \item Jeœli wszystkie instancje zbioru trenuj¹cego s¹ pozytywne, tworzony jest
  wêze³ oznaczaj¹cy sukces i algorytm koñczy dzia³anie -- analogicznie, jeœli
  wszystkie s¹ negatywne, tworzony jest wêze³ oznaczaj¹cy pora¿kê i algorytm siê
  koñczy. W przeciwnym przypadku wybierany jest jeden z atrybutów
  (niedecyzyjnych) i tworzony jest wêze³ decyzyjny.
  \item Zbiór trenuj¹cy dzielony jest na podzbiory wzglêdem wartoœci wybranego w
  kroku 1 atrybutu.
  \item Algorytm dzia³a rekurencyjnie dla ka¿dego z podzbiorów utworzonych w
  kroku 2. O wyborze atrybutu, wzglêdem którego dokonywany jest podzia³ na
  podzbiory decyduje u¿ytkownik.
\end{enumerate}

\emph{ID3} ulepsza algorytm \emph{CLS} poprzez wprowadzenie heurystycznego
wyboru atrybutu, wzglêdem którego dokonywany jest podzia³ zbioru.
Spoœród wszystkich atrybutów niedecyzyjnych w ka¿dym przebiegu algorytmu
wybierany jest ten, który w najlepszy sposób rozdzieli zbiór trenuj¹cy.
Jest to przyk³ad algorytmu zach³annego, za ka¿dym razem wybierany jest bowiem
najlepszy w danym momencie atrybut.

Decyzja o tym, który atrybut jest najlepszy w danym przebiegu algorytmu
podejmowana jest na podstawie tzn. \emph{miary informacji} w zbiorze.
Mierzy ona, jak dobrze dany atrybut dzieli zbiór trenuj¹cy na klasy docelowe.
Atrybut z najwiêksz¹ miar¹ informacji, czyli taki, który jest najbardziej
przydatny z punktu widzenia klasyfikacji, jest wybierany.

%http://www.dcs.napier.ac.uk/~peter/vldb/dm/node11.html

Teoria informacji udziela odpowiedzi na pytanie, który z atrybutów jest
najbardziej przydany. Przypuœcmy, ¿e atrybut decyzyjny $D$ przyjmuje wartoœci
$d_1..d_n$, z kolei aktualnie rozwa¿any atrybut $A$ wartoœci $a_1..a_m$.

Stosunek $p(\frac{d_i}{a_j})$ oznacza prawdopodobieñstwo, ¿e zachodzi $d_i$ pod warunkiem $a_j$.

Teoria informacji definiuje pojêcie \emph{entropii}.

\[
	E = -\sum_{i=1}^{n} p(\frac{d_i}{a_j})log_{2}p(\frac{d_i}{a_j})
\]

Wyra¿enie to okreœla entropiê wzlgêdem atrybutu decyzyjnego D dla konkretnej
wartoœci atrybutu aj. 

Œrednia entropia wyra¿ona wzorem:

\[
	E_{sr} = -\sum_{j=1}^{m} p(a_j) \sum_{i=1}^{n}
	p(\frac{d_i}{a_j})log_{2}p(\frac{d_i}{a_j}) 
\]

oznacza entropiê atrybutu $A$ w stosunku do atrybutu decyzyjnego $D$.

Wyra¿enie to okreœla stopieñ niepewnoœci informacji -- im wy¿sza entropia, tym
mniej pewna jest wartoœæ atrybutu decyzyjnego. Oznacza to, ¿e im atrybut ma
ni¿sz¹ entropiê, tym lepszy jest z punktu widzenia klasyfikacji.

Do zalet tego algorytmu nale¿¹ niew¹tpliwie jego prostota oraz to, ¿e jeœli w
zbiorze treningowym nie ma tzw. ha³asu, czyli rekordów, które dla takich samych
wartoœci atrybutów daj¹ ró¿n¹ decyzjê, to algorytm ten daje zawsze poprawne
wyniki.

Posiada on jednak kilka ograniczeñ -- nie radzi sobie z ci¹g³ymi dziedzinami
atrybutów oraz z niepe³nymi danymi (rekordami wype³nionymi tylko czêœciwo)
a tak¿e drzewo przez niego generowane ma przewa¿nie du¿y rozmiar.
Algorytm ten nie jest tak¿e odporny na zaburzenia danych wejœciowych 
(ha³asu) -- w takim przypadku mo¿e on prowadziæ do wyst¹pienia wysokiego
wspó³czynnika b³êdów na danych testowych.


\paragraph{Algorytm C4.5}

Algorytm \emph{C4.5} jest jednym z najpopularniejszych algorytmów budowy drzew
decyzyjnych. Stworzony zosta³ przez \emph{Quinlan'a} na bazie metody \emph{ID3}
i w zamyœle mia³ usuwaæ ograniczenia swego poprzednika.

W czasie budowy drzewa decyzyjnego algorytm \emph{ID3} nie radzi sobie z
przypadkiem, gdy zbiór danych zawiera niekompletne informacje, czyli gdy rekordy maj¹ nieznan¹ wartoœæ dla pewnych
atrybutów. Algorytm \emph{C4.5} potrafi obs³u¿yæ ten przypadek -- w takiej
sytuacji miara informacji obliczana jest na podstawie atrybutów, dla których
wartoœæ jest zdefiniowana. Podczas przetwarzania przez tak utworzone drzewo zbioru testuj¹cego, mo¿liwe
jest sklasyfikowanie rekordów z nieznan¹ wartoœci¹ pewnych atrybutów poprzez
wyliczenie prawdopodobieñstwa mo¿liwych wyników.

Kolejnym ograniczeniem algortymu \emph{ID3} jest mo¿liwoœæ obs³ugi tylko
atrybutów o wartoœciach dyskretnych. Algorytm \emph{C4.5} radzi sobie tak¿e z
atrybutami przyjmuj¹cymi ci¹g³e wartoœci. W takim przypadku przedzia³ wartoœci
przyjmowany przez dany atrybut $A$ dzielony jest na podprzedzia³y w nastêpuj¹cy
sposób: 

\begin{itemize}
  \item wartoœci atrybutu $A$ szeregowane s¹ rosn¹co ($a_1..a_m$)
  \item dla ka¿dej z wartoœci zbiór rekordów dzielony jest na takie, dla których
  wartoœæ atrybutu $A$ jest mniejsza b¹dŸ równa $a_i$ i wieksza od $a_i$
  \item dla ka¿dego takiego podzia³u wyliczana jest miara informacji i wybierany
jest najbardziej optymalny z nich (dla którego miara informacji jest najwiêksza)
\end{itemize}

Drzewo decyzyjne utworzone za pomoc¹ algorytmu \emph{ID3} mo¿e byæ dosyæ
z³o¿one i mieæ mocno zró¿nicowane pod wzglêdem d³ugoœci ga³êzie, co, wraz z
brakiem odpornoœci na ha³as w danych wejœciowych, prowadzi do dosyæ wysokiego
poziomu b³edów dla rzeczywistych danych.

Jednym z udoskonaleñ tego algorytmu wprowadzonym przez algorytm \emph{C4.5} jest
przycinanie ga³êzi drzewa. Polega ono na zast¹pieniu ca³ego poddrzewa pojedynczym
liœciem. Decyzja o zast¹pieniu poddrzewa dokonywana jest na podstawie porównania
wyników obliczeñ algorytmu heurystycznego dokonywanych na danym poddrzewie i na liœciu.

Dziêki przycinaniu uzyskuje siê wieksz¹ generalizacjê oceny nowych przypadków.

