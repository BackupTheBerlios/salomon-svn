\section{Problemy klasyfikacyjne i~algorytmy wykorzystuj¹ce drzewa decyzyjne}

%An Introduction to~Classification and Regression Tree (CART) Analysis
%Roger J. Lewis, M.D., Ph.D.
%Department of Emergency Medicine
%Harbor-UCLA Medical Center
%Torrance, California, page 2-3

Rozwi¹zywanie zagadnieñ klasyfikacyjnych to~jedno z~zastosowañ algorytmów uczenia maszynowego. Zadania klasyfikacyjne polegaj¹ na~okreœleniu wartoœci jednego z~atrybutów danych (zwanego atrybutem \emph{decyzyjnym}) na~podstawie wartoœci innych atrybutów i~pewnych danych historycznych. Dane historyczne pe³ni¹ rolê danych trenuj¹cych -- algorytm ,,uczy siê'' jakie wartoœci przyjmuje atrybut, którego wartoœæ ma byæ predykowana, w~zale¿noœci od wartoœci pozosta³ych atrybutów. Do~sprawdzenia poprawnoœci predykcji mo¿e zostaæ wykorzystany zbiór danych testuj¹cych, dla którego znana wartoœæ atrybutu decyzyjnego porównywana jest z~wartoœci¹ uzyskan¹ w~wyniku dzia³ania algorytmu.

\subsection{Drzewa decyzyjne}
\label{lab:dec_trees}

Jednym z~przyk³adów reprezentacji wiedzy uzyskanej
za pomoc¹ algorytmów uczenia maszynowego s¹ drzewa decyzyjne.

Przyk³adami takich algorytmów s¹ \emph{ID3} i~\emph{C4.5}, wprowadzone  przez \emph{J. Ross Quinlan'a} do~tworzenia na~podstawie danych indukcyjnych modeli klasyfikuj¹cych, zwanych  drzewami decyzyjnymi.

Jako wejœcie otrzymuj¹ one zbiór rekordów o~jednakowej strukturze, sk³adaj¹cy siê z~pewnej liczby par atrybut-wartoœæ. Jeden z~tych atrybutów reprezentuje
tzw. kategoriê rekordu, jest te¿ okreœlany mianem atrybutu decyzyjnego.

Celem algorytmów jest stworzenie drzewa, które na~podstawie wartoœci
pozosta³ych atrybutów okreœli wartoœæ atrybutu decyzyjnego.
Zwykle atrybut decyzyjny przyjmuje wartoœci logiczne \emph{(prawda, fa³sz)}, 
\emph{(sukces/pora¿ka)} lub podobne. W~ka¿dym przypadku jedna z~wartoœci
atrybutu oznacza pora¿kê.

Drzewa decyzyjne zawdziêczaj¹ sw¹ nazwê temu, ¿e prezentowane s¹ w~formie
struktury drzewiastej (Rys \ref{fig:sample_tree}). Graficzna reprezentacja powoduje,
¿e zrozumienie go, czy te¿ porównanie z~innym drzewem jest proste nawet dla cz³owieka. 
Drzewa decyzyjne sta³y siê bardzo popularne w~technikach eksploracji danych (szerzej opisanych w~rozdziale \ref{lab:data_mining}). 
S¹ one najpowszechniej u¿ywane do~klasyfikacji (predykuj¹ do~której grupy nale¿y dany przypadek), lecz mog¹ byæ równie¿ u¿ywane przy regresji (predykuj¹ wartoœæ).

% The decision tree method encompasses a~number of specific algorithms, including
% Classification and Regression Trees (CART), Chi-squared Automatic Interaction
% Detection (CHAID), C4.5 and C5.0 (from work by J. Ross Quinlan of Rulequest
% Research Pty Ltd, in St. Ives, Australia, www.rulequest.com).

Metody drzew decyzyjnych obejmuj¹ wiele algorytmów. Miêdzy innymi drzewa
klasyfikuj¹ce i~regresyjne (ang. \emph{Classification and Regression Trees
(CART)}), \emph{Chi-squared Automatic
Interaction Detection (CHAID)}, \emph{C4.5} and \emph{C5.0}.

% Decision trees graphically display the relationships found in data. Most
% products also translate the tree-to-text rules such as If Income = High and
% Years on job > 5 Then Credit risk = Good. In fact, decision tree algorithms are
% very similar to~rule induction algorithms which produce rule sets without a
% decision tree.

Drzewa decyzyjne w~graficzny sposób obrazuj¹ zale¿noœci wystêpuj¹ce w~danych.
Mo¿na je równie¿ przedstawiaæ w~postaci regu³ tekstowych np.
\\
\\
\emph{Jeœli Dochód = Wysoki i~Lata Pracy $>$ 5 Wtedy Ryzyko kredytowe = Ma³e}.
\\
\\
Algorytmy drzew decyzyjnych s¹ bardzo podobne do~algorytmów indukcji regu³, które
produkuj¹ zbiory regu³ bez drzew decyzyjnych.

% The primary output of a~decision tree algorithm is the tree itself. The 
% training process that creates the decision tree is usually called induction. 
% Induction requires a~small number of passes (generally far fewer than 100) 
% through the training dataset. This makes the algorithm somewhat less efficient 
% than Naýve-Bayes algorithms (See Naýve-Bayes and Nearest Neighbor.), which 
% require only one pass, but significantly more efficient than neural nets, which 
% typically require a~large number of passes, sometimes numbering in the 
% thousands. To~be more precise, the number of passes required to~build a~
% decision tree is no more than the number of levels in the tree. There is no 
% predetermined limit to~the number of levels, although the complexity of the 
% tree as measured by the depth and breadth of the tree generally increases as 
% the number of independent variables increases.

G³ówny rezultatem algorytmu drzew decyzyjnych jest w³aœnie drzewo. Proces
treningu, który tworzy drzewo, jest zwyczajowy nazywany indukcj¹. Indukcja wymaga kilku
przejœæ (generalnie znacznie mniej ni¿ 100) przez zbiór trenuj¹cy.
Powoduje to, ¿e algorytmy te s¹ mniej wydajne ni¿ algorytmy 
\emph{Naýve-Bayes}, które wymagaj¹ tylko jednego przejœcia, ale znacznie bardziej wydajne
ni¿ sieci neuronowe, które zazwyczaj wymagaj¹ wielkiej iloœci przejœæ, czasami
liczonych w~tysi¹cach. Dok³adniej iloœæ potrzebnych przejœæ wymaganych do
zbudowania drzewa decyzyjnego jest nie wiêksza ni¿ wysokoœæ drzewa (iloœæ
warstw). Nie istnieje okreœlona z~góry maksymalna wysokoœæ drzewa, jednak¿e
z³o¿onoœæ drzewa mierzona jako jego wysokoœæ i~szerokoœæ generalnie roœnie jeœli
roœnie iloœæ niezale¿nych zmiennych.


%\usepackage{graphics} is needed for \includegraphics
\begin{figure}[htp]
\begin{center}
  \includegraphics[width=0.3\textwidth]{img/sample_tree.jpg}
  \caption[labelInTOC]{Drzewo decyzyjne}
  \label{fig:sample_tree}
\end{center}
\end{figure}
