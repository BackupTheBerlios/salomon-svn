
\subsubsection{Algorytm ID3}

%http://www.cise.ufl.edu/~ddd/cap6635/Fall-97/Short-papers/2.htm
Algorytm \emph{ID3} tworzy drzewo decyzyjne na podstawie zbioru danych przyk³adowych.
Utworzone w ten sposób drzewo u¿ywne jest do póŸniejszego klasyfikowania
zbiorów zawieraj¹cych podobne dane.

Ka¿dy liœæ drzewa odpowiada podjêtej decyzji (czyli wartoœci atrybutu
decyzyjnego), natomiast wêz³y reprezentuj¹ testy wartoœci pozosta³ych
atrybutów, wp³ywaj¹cych na decyzjê.

Algorytm \emph{ID3} przedstawiony zosta³ przez \emph{J. Ross Quinlan'a} w roku
1975. Bazuje on na algorytmie \emph{CLS (ang. Concept Learning System)}, który w
uproszczeniu dzia³a nastêpuj¹co:

\begin{enumerate}
  \item Jeœli wszystkie instancje zbioru trenuj¹cego s¹ pozytywne, tworzony jest
  wêze³ oznaczaj¹cy sukces i algorytm koñczy dzia³anie -- analogicznie, jeœli
  wszystkie s¹ negatywne, tworzony jest wêze³ oznaczaj¹cy pora¿kê i algorytm siê
  koñczy. W przeciwnym przypadku wybierany jest jeden z atrybutów
  (niedecyzyjnych) i tworzony jest wêze³ decyzyjny.
  \item Zbiór trenuj¹cy dzielony jest na podzbiory wzglêdem wartoœci wybranego w
  kroku 1 atrybutu.
  \item Algorytm dzia³a rekurencyjnie dla ka¿dego z podzbiorów utworzonych w
  kroku 2. O wyborze atrybutu, wzglêdem którego dokonywany jest podzia³ na
  podzbiory decyduje u¿ytkownik.
\end{enumerate}

\emph{ID3} ulepsza algorytm \emph{CLS} poprzez wprowadzenie heurystycznego
wyboru atrybutu, wzglêdem którego dokonywany jest podzia³ zbioru.
Spoœród wszystkich atrybutów niedecyzyjnych w ka¿dym przebiegu algorytmu
wybierany jest ten, który w najlepszy sposób rozdzieli zbiór trenuj¹cy.
Jest to przyk³ad algorytmu zach³annego, za ka¿dym razem wybierany jest bowiem
najlepszy w danym momencie atrybut.

Decyzja o tym, który atrybut jest najlepszy w danym przebiegu algorytmu
podejmowana jest na podstawie tzn. \emph{miary informacji} w zbiorze.
Mierzy ona, jak dobrze dany atrybut dzieli zbiór trenuj¹cy na klasy docelowe.
Atrybut z najwiêksz¹ miar¹ informacji, czyli taki, który jest najbardziej
przydatny z punktu widzenia klasyfikacji, jest wybierany.

%http://www.dcs.napier.ac.uk/~peter/vldb/dm/node11.html

Teoria informacji udziela odpowiedzi na pytanie, który z atrybutów jest
najbardziej przydany. Przypuœcmy, ¿e atrybut decyzyjny $D$ przyjmuje wartoœci
$d_1..d_n$, z kolei aktualnie rozwa¿any atrybut $A$ wartoœci $a_1..a_m$.

Stosunek $p(\frac{d_i}{a_j})$ oznacza prawdopodobieñstwo, ¿e zachodzi $d_i$ pod warunkiem $a_j$.

Teoria informacji definiuje pojêcie \emph{entropii}.

%TODO wzorek

Wyra¿enie to okreœla entropiê wzlgêdem atrybutu decyzyjnego D dla konkretnej
wartoœci atrybutu aj. 

Œrednia entropia wyra¿ona wzorem:

%TODO: wzorek Esr = \ldots

oznacza entropiê atrybutu $A$ w stosunku do atrybutu decyzyjnego $D$.

Wyra¿enie to okreœla stopieñ niepewnoœci informacji -- im wy¿sza entropia, tym
mniej pewna jest wartoœæ atrybutu decyzyjnego. Oznacza to, ¿e im atrybut ma
ni¿sz¹ entropiê, tym lepszy jest z punktu widzenia klasyfikacji.
